{
  "hash": "41203096d28ff6b9606b7bd351d2e4bf",
  "result": {
    "markdown": "---\ntitle: Lecture 3 - Sampling & Reconstruction\n---\n\n## Summary\n\nIn this lecture, we will continue to refresh understanding on the background material from signals and systems. Specifically, we focus on sampling and reconstruction, multirate signal processing, and related practical aspects for analog-to-digital convertors (ADCs) and digital-to-analog converters (DACs).\n\n## Motivation in Course Context\n\nBased upon the modern radio architecuture introduced in the first lecture, we are now focused on the DACs and ADCs that provide us with the interface between the CT and DT worlds.\n\n![ADC architecture from [https://www.analog.com/en/analog-dialogue/articles/the-right-adc-architecture.html](https://www.analog.com/en/analog-dialogue/articles/the-right-adc-architecture.html)](images/ADC_Application-17.gif){#fig-adi-adc-application}\n\n## Outline\n\n- Analog-to-Digital Converter (ADC) Architecture\n- Digital-to-Analog Converter (DAC) Architecture\n- Sampling and Reconstruction of CT Signals\n    - Frequency-Domain View of Sampling\n    - Ideal Bandlimited Reconstruction and the Sampling Theorem\n    - Aliasing, Nyquist Zones, and Anti-Alias Filtering\n- Decimation and Interpolation of DT Sequences\n    - Downsampling\n    - Interpolation\n    - DT Sampling Theorem\n- Practical Interpolation Filters Require Oversampling\n\n### Analog-to-Digital Conversion\n\n![Analog-to-Digital Converter (ADC) Architecture](images/Lec03-01.png){#fig-adc-architecture}\n\nAt a high level, the ADC architecture in @fig-adc-architecture works well provided the following:\n\n- First, the desired frequency components of the signal $r(t)$ are sufficiently bandlimited, e.g., maximum frequency component less than $f_s/(2M)$.\n\n- Second, the filters $h(t)$ and $g[n]$ are practical approximations to low pass filters (LPFs) that pass the desired frequency content of $r(t)$ and $s[n]$, respectively, and significantly attenuate other undesired frequency components. The wider the transition band of these filters, generally the more bandlimited we require $r(t)$ to be.\n\n- Third, the quantizer does not introduce too much distortion, e.g., $|y-q(y)|<D$ for some small $D$ for \"most\" values of the input $y$.\n\nNote that quantization issues will be addressed in our next lecture.\n\n<!-- Bandlimited signal of finite energy has an amplitude limit. Might be worth using to set signal level relative to quantizer range. -->\n\n### Digital-to-Analog Conversion\n\n![Digital-to-Analog Converter (DAC) Architecture](images/Lec03-02.png){#fig-dac-architecture}\n\nAt a high level, the DAC architecture in @fig-dac-architecture works well provided the following:\n\n- First, the quantizer does not introduce too much distortion, e.g., $|y-q(y)|<D$ for some small $D$ for \"most\" values of the input $y$.\n\n- Second, the input signal $y[n]$ is sufficiently bandlimited that its own periodic images, or the additional ones created through upsampling to produce $x[n]$, do not enter into the transition band or passband of the interpolation filters.\n\n- Third, the filters $g[n]$ and $h(t)$ are practical approximations to low pass filters (LPFs) that pass the desired frequency content of $y[n]$ and $s(t)$, respectively, and significantly attenuate other undesired frequency components. The wider the transition band of these filters, generally the more bandlimited we require $y[n]$ to be.\n\nNote that quantization issues will be addressed in our next lecture.\n\n## Sampling and Reconstruction of CT Signals\n\nThe time domain relationship for sampling is rather simple: given a CT signal $s(t)$ and sampling frequency $f_s > 0$, we define the sampling period as $T_s = 1 / f_s$ and set\n\n$$\ns[n]=\\left. s(t) \\right|_{t=nT_s}=s(nT_s)\n$$ {#eq-samp-time-domain}\n\nThe frequency domain relationship between the two samples that is more complicated.\n\n### Sampling in the Frequency Domain\n\nLet $s(t)\\leftrightarrow S(f)$ be a Fourer transform pair, and let $s[n]=s(n T_s)$, $n\\in\\mathbb{Z}$ for some sampling period $T_s > 0$.\n\nThe frequency domain relationship between the CT signal and the DT sequence is as follows:\n\n$$\nS(e^{j2\\pi u}) = \\frac{1}{T_s} \\sum_{k=-\\infty}^{\\infty} S(\\left (u - k ) / T_s \\right) = f_s \\sum_{k=-\\infty}^{\\infty} S(\\left (u - k)f_s \\right)\n$$ {#eq-ct-dt-fourier}\n\nThis relationship demonstrates that sampling introduces three effects:\n\n1. Amplitude scaling by $1/T_s$\n2. Periodic replication of the spectrum at integer multiples of the sampling frequency, i.e., replicas of $S(f)$ shifted by $kf_s \\mid k\\in\\mathbb{Z}$ and added up\n3. Scaling of the frequecny axis $u = f/f_s$\n\nAfter we illustrate these effects via an example, we provide two alternative ways to develop the mathematical relationship.\n\n#### Illustration\n\nIn the example below, $s(t)$ is a signal bandlimited to maximum frequency $f_m > 0$ and with Fourier transform $S(f)$. \n\n![Illustration of the effects of sampling in the frequency domain.](images/Lec03-03.png){#fig-sampling-fourier-illustration}\n\nWe see from @fig-sampling-fourier-illustration that there is no overlap between the periodic replicas $S(f-kf_s)$, $k\\in\\mathbb{Z}$ in $S_p(f)$ if\n$$\nf_s > 2f_m\n$$ {#eq-nyquist-sampling-condition}\n\n#### Informal Proof\n\nWe define an intermediate CT signal $s_p(t) = s(t)\\cdot p(t)$, where $p(t)$ is an *impulse train*\n\n$$\np(t)=\\sum_{n=-\\infty}^{\\infty} \\delta(t-nT_s)\n$$\n\nso that\n\n$$\ns_p(t) = \\sum_{n=-\\infty}^{\\infty} s(nTs) p(t-nT_s)\n$$\n\nIn the frequency domain, since $p(t)$ is periodic with generalized Fourier transform\n\n$$\nP(f) = \\frac{1}{T_s} \\sum_{k=-\\infty}^{\\infty} \\delta(f-k/T_s)\n$$\n\nconvolution in the frequency domain yields\n\n$$\nS_p(f) = \\frac{1}{T_s} \\sum_{k=-\\infty}^{\\infty} S(f-k/T_s)\n$$\n\nFinally, scaling the time access between CT and DT by $1/T_s$ via $s[n]=s(nT_s)$ corresponds to scaling the frequency axis by $T_s$ via\n\n$$\nS(e^{j2\\pi u})=S_p(u/T_s)=\\frac{1}{T_s} \\sum_{k=-\\infty}^{\\infty} S((u-k)/T_s)\n$$\n\n####  Formal Proof\n\nTo establish the frequency-domain relationship, start with the CT inverse Fourier transform relationship\n$$\ns(t)=\\int_{-\\infty}^{\\infty} S(f) e^{-2\\pi f t} df\n$$\n\nso that\n\n$$\ns[n] = s(nT_s) = \\int_{-\\infty}^{\\infty} S(f) e^{-2\\pi (f/f_s) n} df\n$$\n\nThe change of variables $u=f/f_s$ yields\n\n$$\ns[n] = f_s \\int_{-\\infty}^{\\infty} S(u f_s ) e^{-2\\pi u n} du\n$$\n\nwhich shows $s[n]$ as a superposition of an infinite continuum of DT complex exponentials. However, these complex exponentials only live on the finite normalized frequency axis $u\\in[-1/2,1/2]$ because $e^{-2\\pi (u+k) n}=e^{-2\\pi u n}$ for all $k\\in\\mathbb{Z}$.\n\nWe therefore break up the infinite integral into a sum of integrals, one over each unit interval $[k-1/2,k+1/2]$, $k\\in\\mathbb{Z}$. Specifically,\n\n$$\ns[n] = f_s \\sum_{k=-\\infty}^{\\infty} \\int_{k-1/2}^{k+1/2} S(u f_s ) e^{-2\\pi u n} du\n$$\n\nFinally, we define the changes of variables $v=u-k$ and $l=-k$ so that\n\n$$\ns[n] = f_s \\sum_{k=-\\infty}^{\\infty} \\int_{-1/2}^{+1/2} S((v+k) f_s ) e^{-2\\pi (v+k) n} dv =  \\int_{-1/2}^{+1/2}  \\left[ f_s \\sum_{l=-\\infty}^{\\infty} S((v-l) f_s ) \\right] e^{-2\\pi v n} dv\n$$\n\nThe last relationship is clearly the inverse DT Fourier transform relationship for $s[n]$, so that by inspection we have the desired result\n\n$$\nS(e^{2\\pi v n})=f_s \\sum_{l=-\\infty}^{\\infty} S((v-l) f_s )\n$$ {#eq-sampling-fourier-dt-from-ct}\n\n### Bandlimited Interpolation and the Sampling Theorem\n\nWe say that a signal $s(t)$ is *bandlimited to maximum frequency $f_m$* if $S(f)=0$ for $|f|>f_m$.\n\nThe example and informal proof above suggests that, despite periodic replication through the sampling process, we can recover a bandlimited CT signal from its samples provided there is no overlap of the replicas in the frequency domain, i.e., $f_s > 2 f_m$. This lower limit on the sampling frequency is called the *Nyquist rate* for the bandlimited signal.\n\n#### Bandlimited Interpolation\n\nExact recovery simply consists of filtering the intermediate signal $s_p(t)$ with a lowpass filter of the appropriate cutoff frequency and passband gain, and illustrated in the figure below.\n\n![Illustration of band limited interpolation in the frequency domain.](images/Lec03-04.png){#fig-bandlimited-interpolation}\n\nThe lowpass filter passes and scales the replica centered at $f=0$ and eliminates the replicas centered at $f=kf_s$, $k\\in\\mathbb{Z}-\\{0\\}$.\n\nStrictly speaking, the cutoff frequency for such a filter can be any frequency between $f_m$ and $f_s-f_m$; however, a convention is to set the cutoff frequency to be $f_s / 2$, as illustrated in @fig-bandlimited-interpolation. With this convention, the ideal lowpass filter has frequency response\n\n$$\nH(f)=T_s \\mathrm{rect}(fT_s)\n$$ {#eq-ideal-bandlimited-interpolation-freq-response}\n\nand impulse response\n\n$$\nh(t)=\\mathrm{sinc}(t/T_s)\n$$ {#eq-ideal-bandlimited-interpolation-impulse-response}\n\nThe resulting reconstruction equation is called *bandlimited interpolation*, and takes the form\n\n$$\n\\tilde{s}(t)=s_p(t) * h(t) = \\left(\\sum_{n=-\\infty}^{\\infty} s(nT_s) \\delta(t-nT_s)\\right) * \\mathrm{sinc}(t/T_s) = \\sum_{n=-\\infty}^{\\infty} s(nT_s) \\mathrm{sinc}\\left( \\frac{t-nT_s}{T_s} \\right)\n$$ {#eq-ideal-bandlimited-interpolation}\n\n#### Sampling Theorem\n\nPutting these observations together leads to the following result.\n\n::: {#thm-sampling-theorem}\n\n##### Sampling Theorem\n\nIf a continuous signal $s(t)$ is bandlimited to maximum frequency $f_m > 0$ and sampled with frequency $f_s > 2 f_m$, corresponding to the period $T_s < 1/(2f_m)$, then it can be reconstructed from its $T_s$-spaced samples as:\n\n$$\n\\lim_{N \\rightarrow \\infty} \\sum_{n=-N}^{N} s(nT_s)\\ \\mathrm{sinc}\\left( \\frac{t-nT_s}{T_s} \\right)=s(t)\n$$\n\n:::\n\n### Aliasing, Nyquist Zones, and Anti-Alias Filtering\n\nIf a CT signal $s(t)$ is not bandlimited, or even if the signal is bandlimited but the sampling frequency $f_s$ does not exceed its Nyquist rate, the periodic replicas $S(f-kf_s)$ will overlap in the frequency domain. This effect is called *aliasing*.\n\nAliasing can be better understood if we define the *Nyquist Zones*, which consist of successive intervals of length $f_s/2$ along the CT frequency axis. The Nyquist Zones are illustrated in the figure below.\n\n![Illustration of Nyquist zones.](images/Lec03-05.png){#fig-nyquist-zones}\n\nSpecifically, for positive $k\\in\\mathbb{Z}$, the $k$-th Nyquist Zone is the interval $[(k-1)f_s/2,kf_s/2]$. For negative $k\\mathbb{Z}$, the $k$-th Nyquist Zone is the interval $[kf_s/2,(k+1)f_s/2)$. All Nyquist Zones for $k$ odd and positive alias into the $k=1$ Nyquist Zone, and all Nyquist Zones for $k$ even and positive alias into the $k=-1$ Nyquist Zone. Conversely, all Nyquist Zones for $k$ odd and negative alias into the $k=-1$ Nyquist Zone, and all Nyquist Zones for $k$ even and negative alias into the $k=1$ Nyquist Zone.\n\n<!-- It would seem that there should be a more compact notation to specify and explain these Nyquist Zones. -->\n\nTo avoid aliasing, we often put a lowpass filter in front of the sampler, to limit the frequency content. Ideally, this would be a lowpass filter with cutoff frequency $f_s/2$. Such a filter is called an *anti-alias filter*.\n\n\n## Decimation and Interpolation of DT Sequences\n\nIn this section, we develop similar ideas for changing the sampling rate of DT sequences as for sampling of CT signals. The concepts are largely the same, although the notation is slightly different, and we have to keep in mind that the DT normalized frequency axis is periodic with period $1$.\n\n### Decimation\n\nIn some applications, we have fixed analog hardware that operates at a high sampling rate $f_s$, and then we reduce the effective sampling rate in reprogrammable digital hardware or software. *Decimation* or *Downsampling* by a factor $M\\in\\mathbb{Z}^+$ has input-output relationship\n\n$$\ny[n]=x[nM]\n$$ {#eq-dt-downsampling}\n\nwhich looks very similar to the relationship for sampling a CT signal to produce a DT sequence. The downsampler keeps every $M$-th sample and discards those inbetween.\n\n@fig-downsampling-block-diagram shows a block diagram for the downsampling operation and illustrates a good way to think about its operation relative to CT sampling.\n\n![Block diagram for downsampling relative to CT sampling.](images/Lec03-06.png){#fig-downsampling-block-diagram}\n\nIn particular, we we imagine a CT signal $x(t)$ for which the sequence $x[n]$ represents the samples $x(nT_s)$, downsampling yields the samples $y[n]=x(nMT_s)$, i.e., samples spaced with larger period $MT_s$ or smaller sampling frequency $f_s/M$.\n\nUsing the previous frequency-domain relationships for sampling from CT to DT, we can, after some algebraic manipulations, obtain the following DT relationship for downsampling\n\n$$\nY(e^{j2\\pi u})=\\frac{1}{M} \\sum_{m=0}^{M-1} X(e^{j2\\pi (u-m)/M})\n$$ {#eq-downsampling-courier-domain}\n\nAgain, we have replication of the spectrum, except there are only $M$ replicas within a period of length $1$ in the DT frequency axis.\n\nAs in the CT case, aliasing can occur, we can define DT Nyquist zones, and we can apply anti-alias or decimation filters before downsampling to reduce the effects of aliasing.\n\n### Interpolation\n\nWe often want to recover a DT sequence from its downsampled version, or to increase the effective sampling rate in repgrammable hardware or software, partiularly in order to produce samples for fixed analog hardware that operates at a high sampling rate. This *interpolation* is accomplised in two steps, upsampling followed by filtering.\n\n#### Upsampling\n\n*Upsampling* by a factor $M\\in\\mathbb{Z}^+$ has input-output relationship\n\n$$\nw[n]=\n\\begin{cases}\nx[n/M] & n=0,\\pm M, \\pm 2M, \\ldots \\\\\n0 & \\mathrm{otherwise}\n\\end{cases}\n$$ {#eq-dt-upsampling}\n\nThe upsampler inserts $M-1$ zero samples beteen input samples. Another way to write down this relationship mathematically is\n\n$$\nw[n] = \\sum_{k=-\\infty}^{\\infty} x[kM]\\ \\delta[n-kM]\n$$\n\nwhich is analogous to CT pulse train conversation.\n\n@fig-upsampling-block-diagram shows a block diagram for the upsampling operation and illustrates a good way to think about its operation relative to CT pulse train conversion.\n\n![Block diagram for upsampling relative to CT pulse train conversion.](images/Lec03-07.png){#fig-upsampling-block-diagram}\n\nIn particular, we imagine a DT sequence signal $x[n]$ that we plan to convert back into continuous time with sample spacing $MT_s$. Intead of getting there direclty with an impulse train with spacing $MT_s$, we can achieve the same result by first upsampling and then converting to an impulse train with spacing $T_s$. Equating the two resulting spectra $W_p(f)$, we can derive the input-output relationship for upsampling to be\n\n$$\nW(e^{j2\\pi u})=X(e^{j2\\pi u M})\n$$ {#eq-upsampling-fourier-domain}\n\nSince upsampling scales the time axis by the factor $M$, it scales the frequency axis by the factor $1/M$. This is yet another instance in which we have to be conscious about the periodic nature of the DT frequency axis and the DT Fourier transform. This is illustrated in @fig-upsampling-periodicity-illustration for $M=2$.\n\n![Illustration of periodicity in the frequency domain and upsampling.](images/Lec03-08.png){#fig-upsampling-periodicity-illustration}\n\nImagining that $x[n]$ represents samples of a CT signal $x(t)$, this example illustrates that upsampling $x[n]$ by $2$ is not the same as sampling $x(t)$ at twice the frequency. There are extra frequency components in $X(e^{j2\\pi u 2})$, for example, those that are centered at $\\pm 1/2$, that are called *images*.\n\n#### Interpolation Filtering\n\nTo eliminate the images introduced by upsampling, we again apply a lowpass filter, however, this time in discrete time. For interpolation by a factor of $M$, the convention is to apply an ideal DT lowpass filter with cutoff frequency $1/(2 M)$. The frequency response of this filter is\n$$\nH(e^{j2\\pi u})= M \\mathrm{rect}(u M)\n$$ {#eq-dt-ideal-interpolation-freq-response}\n\nfor $u\\in[-1/2,1/2]$ and periodic with period $1$, and its impulse response is\n\n$$\nh[n] = \\mathrm{sinc}(n /M).\n$$ {#eq-dt-ideal-interpolation-impulse-response}\n\n### DT Sampling Theorem\n\nA DT sequence $x[n]$ is bandlimited to normalized frequency $u_m > 0$ if $X(e^{j2\\pi u})=0$ for $u_m < |u| < 1/2$. Note that this definition is slightly different that in CT, again because the DT frequency axis is periodic with period $1$.\n\n::: {#thm-reconstruction}\n\n##### Reconstruction\n\nIf a DT sequence $x[n]$ is bandlimited to maximum frequency $0 < u_m 1/2$ and downsampled by the integer factor $M$ such that $M < 1/(2u_m)$, then it can be reconstructed through bandlimited interpolation of its $M$-spaced samples as\n\n$$\n\\lim_{K\\rightarrow \\infty} \\sum_{k=-N}^{N} x[kM]\\ \\mathrm{sinc}\\left( \\frac{n-kM}{M} \\right)=x[n]\n$$\n\n:::\n\n### General Sample Rate Conversion\n\nIn many cases, we want to convert to a fractional sampling rate. If the sample rate conversion that we want to achieve is a factor $M / N$, the usual approach is to first interpolate by the factor $M$ and then decimate by the factor $N$.\n\n## Practical Interpolation Filters Require Oversampling\n\nThe final, and practically most important, topic for this lecture addresses practical implementation of interpolation filters. The ideal lowpass filters discussed so far are non-causal and infinite duration, making them impossible to implement in practice.\n\nIn this section we consider more general filters for the CT interpolatio\n$$\n\\tilde{s}(t) = \\mathrm{l.i.m.}_{N\\rightarrow \\infty} \\sum_{n=-N}^{N} s(nT_s)\\ h(t-nT_s).\n$$ {#eq-ct-general-interpolation}\n\nTwo classic examples are the *zero-order hold*, for which $h(t)=\\mathrm{rect}(t/T_s)$, and *linear interpolation*, which is developed in the problem set.\n\nAnother possibility is to time-limit the $\\mathrm{sinc}$ pulse by setting\n\n$$\nh(t)=\\mathrm{rect}(t/(2 L T_s))\\ \\mathrm{sinc}(t/T_s),\n$$\n\nwhich maintains $2L$ sampling periods about $t=0$. This multiplication in the time domain corresponds to convolution in the frequency domain. The smaller we seek to make the integer parameter $L$, and thereby reduce the filtering delay of the interpolation, the wider the dispersion of the ideal lowpass response in the frequency domain.\n\nFor any one of these non-ideal filters, let $f_1$ denote the largest frequency in the passband, and let $f_2$ denote the smallest frequency in the stopband. The band of frequencies $[f_1,f_2]$ is often call the transition band of the lowpass filter.\n\nReturning to our first illustration for a signal $s(t)$ bandlimited to maximum frequency $f_m$, to ensure that the spectrum replica $S(f)$ is passed by the filter, we require $f_1 > f_s$. However, to ensure that the replica $S(f-f_s)$ is not passed by the filter, we require $f_2 < f_s - f_m$. If $f_m$ is fixed, and we want to widen the transition band by increasing $f_2$, then the only way to ensure good reconstruction, i.e., $\\tilde{s}(t) \\approx s(t)$, is to increase the sampling rate $f_s$ as we increase $f_2$. This often translates into $f_s$ being considerably larger than the Nyquist rate $2 f_m$, and is in general called *oversampling*.\n\n<!-- Good place to draw a figure for passband and stopband, illustrate how the periodic replica at $f_s$ has to avoid the transition band. -->\n\n## Quantizer Parameters and Input-Output Relationships\n\nA *quantizer* is a function that takes a continuous-valued input and products a discrete-valued output. It is generally a non-invertible function.\n\n@fig-quantizer-illustration illustrates a fairly general quantizer with $N\\in\\mathbb{Z}^+$ quantization levels. Often $N$ is chosen to be a power of $2$.\n\n![Illustration of a generic quantizer.](images/Lec03-09.png){#fig-quantizer-illustration}\n\n### General Parameterization\n\nMathematically, we define a set of $N+1$ values $b_0 = -\\infty < b_1 < b_2 < \\ldots < b_{N-1} < b_N = \\infty$ along the input axis, and we refer to them as the *quantization region boundaries*. These values partition the input space into the intervals\n\n$$\n\\mathcal{R}_n = (b_{n-1},b_n]\n$$\n\nfor $n=1,2,\\ldots,N$, which are called the *quantization regions*.\n\nWe also define the set of $N$ values $a_1 < a_2 < \\ldots a_N$ along the output axis, and refer to them as the *quantizer representation points*.\n\nThe functional definition of the quantizer is then $q(y) = a_n$ if $y\\in \\mathcal{R}_n$, $n=1,2,\\ldots,N$.\n\nThe minimum representation point $a_1$ and the maximum representation point $a_N$ define the *output range* $a_N-a_1$ of the quantizer.\n\n### Minimum Mean-Square Error Quantization\n\nIf we are given a probability distribution on the input $f_Y(y)$, then we can consider choosing the quantization region boundaries and representation points to minimze $\\mathbb{E}[|q(Y)-Y|^2]$. This is called *minimum mean-square quantization*.\n\nAlthough a solution to this problem does not exist in general, there is an iterative approach that will converge to at least a local minimum that is called the *Lloyd-Max Algorithm*. The iterative algorithm alternates between improving the quantization region boundaries and the representation points. Specifically:\n\n- Given the quantization representation points $\\{a_1,a_2,\\ldots,a_N\\}$, we select the quantization region boundaries according to correspond to the midpoints between representation points, i.e.,\n$$\nb_n = \\frac{a_1+a_n}{2}\n$$\n\nfor $n=1,2,\\ldots,N-1$.\n\n- Given the quantization region boundaries $\\{b_0,b_1,\\ldots,b_N \\}$, we select the quantization representation points to be the conditional mean of the random variable over the quantization region, i.e.,\n\n$$\na_n = \\mathbb{E}[Y | Y \\in \\mathcal{R}_n]\n$$\n\nfor $n=1,2,\\ldots,N$.\n\n### Uniform Quantization\n\nFor many practical implementations, the quantization regions $\\mathcal{R}_2, \\mathcal{R}_3,\\ldots,\\mathcal{R}_{N-1}$ are set to be the same length $\\Delta > 0$, so that the quantization region boundaries satisfy\n\n$$\nb_n = b_{k-1} + \\Delta\n$$ {#eq-uniform-quantization-boundaries}\n\nfor $k=2,3,\\ldots,N_1$.\n\nFurthermore, the representation points of the internal quantization regions are the midpoints of these intervals. Specifically,\n\n$$\na_n =\n\\begin{cases}\n\\frac{b_n + b_{n-1}}{2} = b_{n-1} + \\frac{\\Delta}{2} & n=2,\\ldots,N-2 \\\\\nb_n - \\frac{\\Delta}{2} & n= 1 \\\\\nb_n + \\frac{\\Delta}{2} & n=N-1\n\\end{cases}\n$$ {#eq-uniform-quantization-points}\n\nThe output range of such a quantizer is then $a_N - a_1 = (N-1)\\Delta$.\n\nWe can define a standard, $K$-bit uniform quantizer with output range $1$ as the function\n\n$$\nq_K(y) =\n\\begin{cases}\n\\lfloor r \\left( y + \\frac{1}{2} + 1/(2r) \\right) \\rfloor / r - \\frac{1}{2} & -\\frac{1}{2} < y < +\\frac{1}{2} \\\\\n+\\frac{1}{2} & y \\ge +\\frac{1}{2}\\\\\n-\\frac{1}{2} & y \\le -\\frac{1}{2}\n\\end{cases}\n$$ {#eq-uniform-k-bit-quantizer}\n\nwhere $r=2^K-1$. For convenience, the quantizer representation points can be easily represented the integers $0,1,\\ldots,2^K-1$ or their binary representations.\n\nThis quantizer limits the input to $\\pm 1/2$. If we want to limit the input and output to $\\pm A$, then we can scale the quantizer by applying $2A q_K(y / (2A))$.\n\nThe input-output relationships for this standardized and scaled standardized uniform quantizer as illustrated in the Python code below.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef q(y,K):\n    z = np.zeros(np.size(y))\n    for i in range(np.size(z)):\n        if y[i] >= 0.5:\n            z[i] = 0.5\n        elif y[i] <= -0.5:\n            z[i] = -0.5\n        else:\n            r = np.power(2.0,K)-1.0\n            z[i] = np.floor( r*(y[i]+0.5 + 1.0/r/2.0 ) ) / r - 0.5\n    return z\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nK = 4                   # 4 bits, 16 levels\nM = 100*np.power(2.0,K) # 100 samples per quantization region\nA = 1                # Expand range of quantizer to \\pm A\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ny_values = np.arange(-2*A,+2*A,A/M)\nqKy = 2.0*A*q(y_values/(2.0*A),K)\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nplt.plot(y_values,qKy,'-')\nplt.plot(y_values,y_values,'-.')\nplt.xlabel(\"$y$\")\nplt.ylabel(\"$2 A q_K(y/(2A))$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Lec03-Sampling_files/figure-html/cell-6-output-1.png){width=602 height=422}\n:::\n:::\n\n\n## Effect of Uniform Quantization on Sinusoidal Signals\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nu0 = 1/4\nn = np.arange(0,128.0/u0,1)\ny = np.cos(2*np.pi*u0*n)\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nK=8\nA=2.5\nyq=2.0*A*q(y/(2.0*A),K)\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nnfft=512\nu=np.fft.fftshift(np.fft.fftfreq(nfft))\nY=np.fft.fftshift(np.fft.fft(y,nfft))\nYq=np.fft.fftshift(np.fft.fft(yq,nfft))\nplt.plot(u,10.0*np.log10(np.abs(Y)**2),'-')\nplt.plot(u,10.0*np.log10(np.abs(Yq)**2),'--')\nplt.xlabel(\"$u$\")\nplt.ylabel(\"$Y(e^{j2\\pi u})$ (dB)\")\nplt.axis([-1/2.0, 1/2.0, -80.0, 60.0])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Lec03-Sampling_files/figure-html/cell-9-output-1.png){width=600 height=427}\n:::\n:::\n\n\n## Take Aways\n\n- Quantization converts a continuous-amplitude input into a discrete-valued output, and it inherently introduces some distortion.\n\n- It is important to consider the output range of the quantizer relative to our input signal amplitude to ensure that we obtain a low error representation\n    - If the input signal amplitude is too small, quantization noise will dominate the output.\n    - If the input signal amplitude is too large, clipping distortion will dominate the output.\n\n",
    "supporting": [
      "Lec03-Sampling_files"
    ],
    "filters": [],
    "includes": {}
  }
}